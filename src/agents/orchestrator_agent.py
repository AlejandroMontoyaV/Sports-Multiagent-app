from langchain_core.tools import BaseTool
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.agents import create_tool_calling_agent, AgentExecutor
from typing import List


# Aqui todos los dem치s agentes se van a considerar como tools
class OrchestratorAgent:
    """
    Agente orquestador del sistema.

    - Recibe la pregunta del usuario.
    - Decide qu칠 tools llamar (clasificador, RAG, evaluador, etc.).
    - Usa las respuestas de los tools para construir una respuesta final en espa침ol.
    """

    def __init__(self, llm: BaseChatModel, tools: List[BaseTool]):
        """
        Par치metros
        ----------
        llm : BaseChatModel
            Modelo de lenguaje que actuar치 como agente orquestador.
        tools : List[BaseTool]
            Lista de tools de LangChain disponibles (index_documents, classify_query, etc.).
        """
        self.llm = llm
        self.tools = tools

        # Prompt del agente orquestador
        self.prompt = ChatPromptTemplate.from_messages([
            ("system",
                """
            Eres el AGENTE ORQUESTADOR de un sistema de pregunta-respuesta sobre DEPORTES.

            Dispones de varias herramientas (tools) que puedes invocar cuando lo necesites.
            Tu objetivo es ayudar al usuario respondiendo SIEMPRE en espa침ol de forma clara
            y correcta, utilizando las herramientas de manera razonada.

            Herramientas principales (resumen):
            - index_documents(): indexa los documentos en el 칤ndice FAISS (칰sala al inicio o si el 칤ndice no est치 actualizado). Esta solo se ejecuta si se a침aden nueos documentos
            - classify_query(query): clasifica la intenci칩n de la consulta del usuario. Las respuestas van a ser "busqueda", "resumen", "comparaci칩n" 칩 "general"
            - retrieve_documents(query): obtiene fragmentos relevantes desde los documentos.
            - answer_with_rag(query): genera una respuesta basada en los documentos recuperados (RAG). Devuelve un JSON con "answer" y "context".
            - evaluate_answer(query, rag_result_json): eval칰a la respuesta del RAG usando el contexto y devuelve un JSON con "veredicto" y "explicacion".

            Flujo recomendado:
            1. Para una nueva consulta del usuario:
            - Usa primero classify_query(query) para entender el tipo de pregunta.
            2. Si la consulta requiere informaci칩n basada en documentos("busqueda", "resumen", "comparaci칩n"):
            - Llama a answer_with_rag(query) para obtener una respuesta propuesta.
            - A continuaci칩n, llama a evaluate_answer(query, rag_result_json) con el JSON que devolvi칩 answer_with_rag.
            - Si el veredicto es "RECHAZAR", puedes intentar UNA segunda vez con answer_with_rag
                y volver a evaluar. Si a칰n as칤 sale "RECHAZAR", expl칤cale al usuario que no puedes
                dar una respuesta fiable basada en los documentos.
            3. Si la respuesta no requiere informaci칩n basada en documentos ("general"), responde con lo que te de el LLM
            4. Solo cuando est칠s satisfecho con la informaci칩n de las herramientas,
            responde al usuario de forma directa y clara.

            Muy importante:
            - NO inventes herramientas nuevas: usa solo las que est치n disponibles.
            - NO hagas suposiciones fuertes sin apoyo de los tools.
            - La respuesta final al usuario debe ser un texto en espa침ol, sin JSON.
            - Muestra las ideas de manera ordenada y, si es 칰til, menciona de forma natural
            que tu respuesta est치 basada en los documentos del sistema.
                            """.strip()
            ),
            # Mensaje del usuario
            ("human", "{input}"),
            MessagesPlaceholder("agent_scratchpad"),
        ])

        # Creamos el agente de tool-calling y su ejecutor
        self.agent = create_tool_calling_agent(self.llm, self.tools, self.prompt)
        self.executor = AgentExecutor(agent=self.agent, tools=self.tools, verbose=False, return_intermediate_steps=True)

    def run(self, query: str) -> str:
        """
        Ejecuta el agente orquestador con la consulta del usuario.
        """
        result = self.executor.invoke({"input": query})
        pasos = result.get("intermediate_steps", [])

        print("\n游댌 Pasos intermedios:")
        for paso in pasos:
            tool_call, tool_output = paso
            print(f"- Tool: {tool_call.tool}, args: {tool_call.tool_input}")
            print(f"  Output: {tool_output}\n")
        
        return result.get("output", "")